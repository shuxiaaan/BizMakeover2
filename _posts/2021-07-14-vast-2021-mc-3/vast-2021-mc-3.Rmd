---
title: "Vast 2021 MC 3"
description: |
  Visual analytics assignment: mini challenge 3
author:
  - name: Li Shuxian
date: 07-14-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

#### import packages
```{r}


packages= c()

packages = c('DT','tidytext','widyr','dplyr','wordcloud',
             'ggwordcloud','textplot','lubridate','hms','tidyverse','tidygraph',
             'ggraph','igraph','LDAvis','servr','dplyr','stringi',
             'raster','sf','clock','tmap','data.table','textclean','tm',
             'wordcloud','wordcloud2','text2vec','topicmodels','tidytext',
             'textmineR','quanteda','BTM','textplot','concaveman','ggwordcloud',
             'qdapDictionaries','textstem','devtools','ggiraph',
             'plotly','igraph', 'tidygraph','visNetwork','udpipe','grid',
             'SnowballC','proustr')

for (p in packages){
  if (!require(p,character.only = T)){
    install.packages(p)    
  }
  library(p,character.only = T)
}
```
#### import data
```{r,echo=TRUE,eval=TRUE}
segment1 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1700-1830.csv')
segment2 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1831-2000.csv')
segment3 <-read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-2001-2131.csv')
```
#### combine all these 3 segments data together 
```{r,echo=TRUE,eval=TRUE}

raw_text <- rbind(segment1,segment2,segment3)
```

#### Wrangling time
```{r,echo=TRUE,eval=TRUE}
raw_text$`date(yyyyMMddHHmmss)`<-ymd_hms(raw_text$`date(yyyyMMddHHmmss)`)
```

#### 1.1 clean data,raw_text
```{r,echo=TRUE,eval=TRUE}
raw_text$clean_message <-raw_text$message%>%
  tolower()%>%#change all messages to lowercase
  replace_contraction()%>%#remove short form
  replace_word_elongation()%>% #remove the same letter appears unnecessarily, eg.'loooook' to 'look'
  str_squish()%>% #re3moves space from start and end of string
  lemmatize_strings()%>%#perform lemmatization
  removeWords(stopwords('english'))#%>%#remove stopwords
```

#### 1.2 clean data,remove keywords in the message - these messages are identified as junk messages
```{r,echo=TRUE,eval=TRUE}
raw_text$clean_message <-raw_text$message %>% 
  #remove rt @ in the message, replace with""
  str_replace_all("RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)","")%>%
  str_replace_all("rt @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove @ in the message, replace with""
  str_replace_all("@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove # in the message, replace with""
  str_replace_all("#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove stop word:the/The/to/ of / is/ in /you /and/ have/ at /are /for/ on/your/ it/ that /be with /more 
  gsub(pattern ='The',replacement = "",raw_text$message)%>%
  gsub(pattern ='the',replacement = "",raw_text$message)%>%
  gsub(pattern ='to',replacement = "",raw_text$message)%>%
  gsub(pattern ='of',replacement = "",raw_text$message)%>%
  gsub(pattern ='is',replacement = "",raw_text$message)%>%
  gsub(pattern ='in',replacement = "",raw_text$message)%>%
  gsub(pattern ='you',replacement = "",raw_text$message)%>%
  gsub(pattern ='your',replacement = "",raw_text$message)%>%
  gsub(pattern ='and',replacement = "",raw_text$message)%>%
  gsub(pattern ='have',replacement = "",raw_text$message)%>%
  gsub(pattern ='at',replacement = "",raw_text$message)%>%
  gsub(pattern ='are',replacement = "",raw_text$message)%>%
  gsub(pattern ='for',replacement = "",raw_text$message)%>%
   gsub(pattern ='on',replacement = "",raw_text$message)%>%
  gsub(pattern ='it',replacement = "",raw_text$message)%>%
  gsub(pattern ='that',replacement = "",raw_text$message)%>%
  gsub(pattern ='be',replacement = "",raw_text$message)%>%
  gsub(pattern ='with',replacement = "",raw_text$message)%>%
  gsub(pattern ='more',replacement = "",raw_text$message)
```

#### 1.3 stemming message
```{r,echo=TRUE,eval=TRUE}
pr_stem_words(raw_text,clean_message,language = "english")
```
#### 1.4 split into blog and text transcripts of emergency call
```{r,echo=TRUE,eval=TRUE}
blog <- filter(raw_text,type=='mbdata')
call <- filter(raw_text,type=='ccdata') 
```

#### 1.5 data explore: text mining and word cloud for blog
```{r,echo=TRUE,eval=TRUE}
#text transform: convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(blog$clean_message)))
inspect(docs[1:2])
#build a term-document matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#generate word cloud
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 10,max.words=300,
          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```

#### There are some stopword should be removed seeing from the word cloud graph. They are: to, of, a, i,s in, you, and, have, at, are, for, on, i, your, it, that, be, with, more. 

#### 1.6 data explore: text mining and word cloud for call
```{r,echo=TRUE,eval=TRUE}
#text transform: convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(call$clean_message)))
inspect(docs[1:2])
#build a term-document matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#generate word cloud
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 5,max.words=100,
          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```
### Question 1. Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.
```{r,echo=TRUE,eval=TRUE}
#create new column: ID and Time, bin time for every hour for blog
blog$ID <- seq.int(nrow(blog))
blog$time_bin = cut(blog$`date(yyyyMMddHHmmss)`, breaks="60 mins")
blog$time_bin<-blog$time_bin %>% str_replace_all("2014-01-23","")

blog_topic<-blog%>%
  group_by(time_bin) %>% 
  unnest_tokens(word, clean_message) %>%
  count(word, sort = TRUE)
```


```{r,echo=TRUE,eval=TRUE}
#draw a correlation graph to see what are the timing that events are strongly correlated
blog_cors <- blog_topic %>% pairwise_cor(time_bin,word,n,sort = TRUE)

set.seed(1234)
blog_cors %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation,width = correlation)) +
  geom_node_point(size = 6,color = "blue") +
  geom_node_text(aes(label = name),color = "red",repel = TRUE) +
  ggtitle("Correlation between time")+
  theme_void()
```
The graph above shows the strong correlation from 6pm to 9pm. Overall, the correlation in average is higher than 0.8. It shows the importance of time in the event. We can narrow down the focused time period from 5-9pm to 6-9pm now.

#### Now we look at the frequent words in each hour and try to extract inccident from it.
```{r,echo=TRUE,eval=TRUE}
tf_idf <- blog_topic%>%
  bind_tf_idf(word,time_bin, n) %>%
  arrange(desc(tf_idf))

tf_idf %>%
  group_by(time_bin) %>%
  slice_max(tf_idf,n =10) %>%
  ungroup() %>%
  mutate(word = reorder(word,tf_idf)) %>%
  ggplot(aes(tf_idf,word,fill = time_bin)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ time_bin,scales = "free",as.table=TRUE) +
  labs(x ='mean tf-idf_score',y= NULL) +
  ggtitle("Blog Term Frequency by hour")
```
#### From the TF bar chart above,there are a few events we can make a guess.  
#### Event 1,there was a fire happened at appartment: dancing dolphin at 6 pm.Neighborhood evacuations started around 7pm. Firefighter came around 8pm and was injured. The fire caused explosion at 9pm.Suspects of arsonist were arrested.Event 2,there was a shot fired by officer at 7 pm.Event 3, probably there was a car/van accident at 7pm.



### word cloud by hour for blog
```{r,echo=TRUE,eval=TRUE}
set.seed(1234)
blog_topic %>%
  group_by(time_bin) %>% 
  slice_max(order_by = n, n = 100) %>% 
ggplot(aes(label = word,size = n,col = as.character(n))) +
  geom_text_wordcloud() +
  theme_minimal() +
  ggtitle("Blog:Word Cloud by hour")+
  facet_wrap(~time_bin)
```
From the word cloud above, words like "the","and","a","is" are ignored. 
All these keywords are supported evidence for those events mentioned previously.
1."Pokrally" is certainly a important word over the hours. Given the background information, it is a gathering from organization Protectors of Kronos.This is one more event detected from word cloud.
2."Kronosstar" and "abilapost" are highly frequent term. It is an author name which means it is either he post a lot or he is an influencer and being retweet a lot.


#### Extract per-topic-per-word probabilities Î²/beta from the model. The higher the value, the more significant the words are to the topic.Then run a topic modeling LDA to extract keywords.Use gamma to assign each document to a topic. 
```{r,echo=TRUE,eval=TRUE}
blogDTM <-blog_topic%>%cast_dtm(time_bin,word,n)
blogLDA <-LDA(blogDTM, k = 20, control = list(seed = 1234))
topicProb <- tidy(blogLDA, matrix = "beta")
topicProb
```
```{r,echo=TRUE,eval=TRUE}
blogGamma<-tidy(blogLDA, matrix = "gamma")%>%group_by(document)
blogGamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  ggtitle("Blog:Topic Distribution over time")+
  facet_wrap(~ title)
```  
This graph help us to determine the influence of each event.It is determined significant topic when their gamma score is greater than 0.3. At 5 pm, topic 6,9,19 are the ones. Extract useful words from topic them,we can determine it is pokrally (start of event 5).At 6pm, topic 13 shoots to the top and reaches peak among the 4 hours. The word "abila" raised the attention. They are participants for event 2 (fire).At 6pm, topic 3,8,10 are hot topics. Key words are "pokrally" and "kronosstar".At 7pm, topic 1 and 13 are trendy topics which has the same content as previous.

```{r,echo=TRUE,eval=TRUE}
Topic <- topicProb %>%
  group_by(topic) %>%
  slice_max(beta, n =5) %>% 
  ungroup() %>%
  arrange(topic, -beta)
```

```{r,echo=TRUE,eval=TRUE}
Topic %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  ggtitle("Blog:Topic Modeling")+
  warnings()
```
Crime: appear in topic 1,2,3,5,15,16.
Traffic: appear in topic 1,2,4,8,9,10,11,12,14,17,18,19,20.
There are multiple words appear in multiple topics. It shows the importance to plot the graph by time. 
Between 6-7pm, topic 13 shows in the peak of the graph. Key words about the car accidents are extracted: a black van vehicle hit someone/a car.

### Repeat the same step for calls.
```{r,echo=TRUE,eval=TRUE}
#create new column: ID and Time, bin time for every hour for call
call$ID <- seq.int(nrow(call))
call$time_bin = cut(call$`date(yyyyMMddHHmmss)`, breaks="60 mins")
call$time_bin<-call$time_bin %>% str_replace_all("2014-01-23","")

call_topic<-call%>%
  group_by(time_bin) %>% 
  unnest_tokens(word, message) %>%
  count(word, sort = TRUE)
```


### word cloud by hour for call
```{r,echo=TRUE,eval=TRUE}
set.seed(1234)
call_topic %>%
  group_by(time_bin) %>% 
  slice_max(order_by = n, n = 20) %>% 
ggplot(aes(label = word,size = n,col = as.character(n))) +
  geom_text_wordcloud() +
  theme_minimal() +
  ggtitle("Call:Word Cloud by hour")+
  facet_wrap(~time_bin)
```
From word cloud of emergency call, the word "stop" appeared frequently. It is assumed people who stayed nearby dancing dolphin might call fire fighting hotline to stop the fires.
 
```{r,echo=TRUE,eval=TRUE}
callDTM <-call_topic%>%cast_dtm(time_bin,word,n)
callLDA <-LDA(callDTM, k = 20, control = list(seed = 1234))
topicProb <- tidy(callLDA, matrix = "beta")
topicProb

callGamma<-tidy(callLDA, matrix = "gamma")%>%group_by(document)
callGamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  ggtitle("Call:Topic Distribution over time")+
  facet_wrap(~ title)
```
To conclude insights from question 1, there are 4 events found: pok rally starts at 5pm. Fire started between 6 to 7pm at dancing dolphin. After 7pm, evacuations started as the fire got worst. Even the fire fighter injured. Meanwhile, a black van hit a car caused accident.Lastly, the fire caused explosion and suspects for the fire was arrested. 

#### Qn2:Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.

###### Zoom in to analyze the changes in the minute unit instead of hour.
```{r,echo=TRUE,eval=TRUE}
blog$mins = cut(blog$`date(yyyyMMddHHmmss)`, breaks="10 min")
blog$mins<-blog$mins %>% str_replace_all("2014-01-23","")
```

```{r,echo=TRUE,eval=TRUE}
count<-blog%>%
  group_by(mins) %>%
  summarise(number = n())
datatable(count,class='mins',style = "default", width =NULL, height = NULL,
          elementId = NULL,colnames=c('Time', 'No of Blog Posted'))
```
Number of blog posted over time is shown above. Intensive discussion happened in 5:10-5:20 pm. It increased rapidly in 6:40 - 7:00 which is the time fire started.7:40pm, blog posted exceed 400 and it is the highest among all. 

```{r,echo=TRUE,eval=TRUE}
call$mins = cut(call$`date(yyyyMMddHHmmss)`, breaks="10 min")
call$mins<-call$mins %>% str_replace_all("2014-01-23","")
```

```{r,echo=TRUE,eval=TRUE}
count<-call%>%
  group_by(mins) %>%
  summarise(number = n())
datatable(count,class='mins',style = "default", width =NULL, height = NULL,
          elementId = NULL,colnames=c('Time', 'No of Emergency Call'))
```
Rank number of calls in descending order, it is found that most of the calls made around 7:40pm. This is the timing where gunshot or the car accident happened.

###### To continue,I would like to explore distraction impact of these events from pok rally. These selected authors are members from POK such as POK leader Sylvia. 
```{r,echo=TRUE,eval=TRUE}
POKrally<-blog%>%filter(str_detect(author,'KronosStar')|
                              str_detect(author,'POK')|
                              str_detect(author,'AbilaPost')|    
                              str_detect(author,'FriendsOfKronos'))%>%
  group_by(mins) %>%
  summarise(number = n()) 
plot_ly(data = POKrally,
        x = ~mins,
        y = ~number,
        marker = list(color = "orange"),
        colors = "black")%>%
  layout(title = 'Pok Park Rally Event')
```

```{r,echo=TRUE,eval=TRUE}
FIRE<-blog%>%filter(str_detect(message,'dancingdolphin')|
                              str_detect(message,'dancing dolphin')|
                              str_detect(message,'DancingDolphin')|    
                              str_detect(message,'Dancing Dolphin'))%>%
  group_by(mins) %>%
  summarise(number = n()) 
plot_ly(data = FIRE,
        x = ~mins,
        y = ~number,
        marker = list(color = "black"),
        colors = "dark green")%>%
  layout(title = 'Fire at Dancing Dolphin Event')

```
```{r,echo=TRUE,eval=TRUE}
Gun<-blog%>%filter(str_detect(message,'gun fire')|
                              str_detect(message,'gunfire')|
                              str_detect(message,'Gun Fire')|    
                              str_detect(message,'GUNFIRE'))%>%
  group_by(mins) %>%
  summarise(number = n()) 
plot_ly(data = FIRE,
        x = ~mins,
        y = ~number,
        colors = "dark ORANGE")%>%
  layout(title = 'Gunfire Event')

```
```{r,echo=TRUE,eval=TRUE}
VAN<-blog%>%filter(str_detect(message,'VAN')|str_detect(message,'van'))%>%
  group_by(mins) %>%
  summarise(number = n()) 
plot_ly(data = VAN,
        x = ~mins,
        y = ~number,
        marker = list(color = "green"),
        colors = "dark green")%>%
  layout(title = 'Black Van Accident Event')

```

![](image/post.png){width=100%}
From the image above, 


