---
title: "Vast 2021 MC 3"
description: |
  A short description of the post.
author:
  - name: Li Shuxian
    url: https://example.com/norajones
date: 07-14-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Distill is a publication format for scientific and technical writing, native to the web.

Learn more about using Distill at <https://rstudio.github.io/distill>.

```{r}
#import packages

packages= c()

packages = c('DT','tidytext','widyr','dplyr','wordcloud',
             'ggwordcloud','textplot','lubridate','hms','tidyverse','tidygraph',
             'ggraph','igraph','LDAvis','servr','dplyr','stringi',
             'raster','sf','clock','tmap','data.table','textclean','tm',
             'wordcloud','wordcloud2','text2vec','topicmodels','tidytext',
             'textmineR','quanteda','BTM','textplot','concaveman','ggwordcloud',
             'qdapDictionaries','textstem','devtools','ggiraph',
             'plotly','igraph', 'tidygraph','visNetwork','udpipe','grid',
             'SnowballC','proustr')

for (p in packages){
  if (!require(p,character.only = T)){
    install.packages(p)    
  }
  library(p,character.only = T)
}
```

```{r,echo=TRUE,eval=TRUE}
segment1 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1700-1830.csv')
segment2 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1831-2000.csv')
segment3 <-read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-2001-2131.csv')
```

```{r,echo=TRUE,eval=TRUE}
#combine all these 3 segments data together 
raw_text <- rbind(segment1,segment2,segment3)
```

```{r,echo=TRUE,eval=TRUE}
#Wrangling time
raw_text$`date(yyyyMMddHHmmss)`<-ymd_hms(raw_text$`date(yyyyMMddHHmmss)`)
```

```{r,echo=TRUE,eval=TRUE}
#1.clean data,raw_text
raw_text$clean_message <-raw_text$message%>%
  tolower()%>%#change all messages to lowercase
  replace_contraction()%>%#remove short form
  replace_word_elongation()%>% #remove the same letter appears unnecessarily, eg.'loooook' to 'look'
  str_squish()%>% #re3moves space from start and end of string
  lemmatize_strings()%>%#perform lemmatization
  removeWords(stopwords('english'))#%>%#remove stopwords
```

```{r,echo=TRUE,eval=TRUE}
#2.clean data,remove keywords in the message
raw_text$clean_message <-raw_text$message %>% 
  #remove rt @ in the message, replace with""
  str_replace_all("RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove @ in the message, replace with""
  str_replace_all("@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove # in the message, replace with""
  str_replace_all("#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\.)","")%>%
  #remove stop word:the/The
  gsub(pattern ='The',replacement = "",raw_text$message)%>%
  gsub(pattern ='the',replacement = "",raw_text$message)
```


```{r,echo=TRUE,eval=TRUE}
#stemming message
pr_stem_words(raw_text,clean_message,language = "english")
```
#split into blog and text transcripts of emergency call
```{r,echo=TRUE,eval=TRUE}
blog <- filter(raw_text,type=='mbdata')
call <- filter(raw_text,type=='ccdata') 
```

#2. text mining and word cloud for blog
```{r,echo=TRUE,eval=TRUE}
#text transform: convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(blog$clean_message)))
inspect(docs[1:2])
#build a term-document matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#generate word cloud
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 10,max.words=300,
          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```
# 2. text mining and word cloud for call
```{r,echo=TRUE,eval=TRUE}
#text transform: convert dataframe to corpus
docs <- Corpus(VectorSource(as.character(call$clean_message)))
inspect(docs[1:2])
#build a term-document matrix
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
# words and frequency dataframe
df <- data.frame(word = names(words),freq=words)
#generate word cloud
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 5,max.words=100,
          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```

```{r,echo=TRUE,eval=TRUE}
#create new column: ID and Time, bin time for every 15 minutes
blog$ID <- seq.int(nrow(blog))
blog$time_bin = cut(blog$`date(yyyyMMddHHmmss)`, breaks="60 mins")
blog$time_bin<-blog$time_bin %>% str_replace_all("2014-01-23","")

blog_topic<-blog%>%
  group_by(time_bin) %>% 
  unnest_tokens(word, message) %>%
  count(word, sort = TRUE)
```

# The graph below shows the strong correlation from 6pm to 9pm. Overall, the correlation in average is higher than 0.8. It shows the importance of time in the event. We can narrow down the focused time period from 5-9pm to 6-9pm now.
```{r,echo=TRUE,eval=TRUE}
#draw a correlation graph to see what are the timing that events are strongly correlated
blog_cors <- blog_topic %>% pairwise_cor(time_bin,word,n,sort = TRUE)

set.seed(1234)
blog_cors %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation,width = correlation)) +
  geom_node_point(size = 6,color = "blue") +
  geom_node_text(aes(label = name),color = "red",repel = TRUE) +
  ggtitle("Correlation between time")+
  theme_void()
```
# Now we look at the frequent words in each hour and try to extract inccident from it.
```{r,echo=TRUE,eval=TRUE}
tf_idf <- blog_topic%>%
  bind_tf_idf(word,time_bin, n) %>%
  arrange(desc(tf_idf))

tf_idf %>%
  group_by(time_bin) %>%
  slice_max(tf_idf,n =10) %>%
  ungroup() %>%
  mutate(word = reorder(word,tf_idf)) %>%
  ggplot(aes(tf_idf,word,fill = time_bin)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ time_bin,scales = "free",as.table=TRUE) +
  labs(x ='mean tf-idf_score',y= NULL) +
  ggtitle("Term Frequency by hour")
```


```{r,echo=TRUE,eval=TRUE}
blog_dtm<- blog_topic %>%
  cast_dtm(time_bin,word,n)
LDA <- LDA(blog_dtm, k = 10, control = list(seed = 1234))
ap_topics <- tidy(LDA, matrix = "beta")
ap_topics
```




```

```{r,echo=TRUE,eval=TRUE}
```

