[
  {
    "path": "posts/2021-07-14-vast-2021-mc-3/",
    "title": "Vast 2021 MC 3",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n\r\n\r\nsegment1 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1700-1830.csv')\r\nsegment2 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1831-2000.csv')\r\nsegment3 <-read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-2001-2131.csv')\r\n\r\n\r\n\r\n\r\n\r\n#combine all these 3 segments data together \r\nraw_text <- rbind(segment1,segment2,segment3)\r\n\r\n\r\n\r\n\r\n\r\n#Wrangling time\r\nraw_text$`date(yyyyMMddHHmmss)`<-ymd_hms(raw_text$`date(yyyyMMddHHmmss)`)\r\n\r\n\r\n\r\n\r\n\r\n#1.clean data,raw_text\r\nraw_text$clean_message <-raw_text$message%>%\r\n  tolower()%>%#change all messages to lowercase\r\n  replace_contraction()%>%#remove short form\r\n  replace_word_elongation()%>% #remove the same letter appears unnecessarily, eg.'loooook' to 'look'\r\n  str_squish()%>% #re3moves space from start and end of string\r\n  lemmatize_strings()%>%#perform lemmatization\r\n  removeWords(stopwords('english'))#%>%#remove stopwords\r\n\r\n\r\n\r\n\r\n\r\n#2.clean data,remove keywords in the message\r\nraw_text$clean_message <-raw_text$message %>% \r\n  #remove rt @ in the message, replace with\"\"\r\n  str_replace_all(\"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove @ in the message, replace with\"\"\r\n  str_replace_all(\"@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove # in the message, replace with\"\"\r\n  str_replace_all(\"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove stop word:the/The\r\n  gsub(pattern ='The',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='the',replacement = \"\",raw_text$message)\r\n\r\n\r\n\r\n\r\n\r\n#stemming message\r\npr_stem_words(raw_text,clean_message,language = \"english\")\r\n\r\n\r\n# A tibble: 4,063 x 8\r\n   type   `date(yyyyMMddHHmm~ author  message       latitude longitude\r\n * <chr>  <dttm>              <chr>   <chr>            <dbl>     <dbl>\r\n 1 mbdata 2014-01-23 17:00:00 POK     \"Follow us @~       NA        NA\r\n 2 mbdata 2014-01-23 17:00:00 maha_H~ \"Don't miss ~       NA        NA\r\n 3 mbdata 2014-01-23 17:00:00 Viktor~ \"Come join u~       NA        NA\r\n 4 mbdata 2014-01-23 17:00:00 Kronos~ \"POK rally t~       NA        NA\r\n 5 mbdata 2014-01-23 17:00:00 AbilaP~ \"POK rally s~       NA        NA\r\n 6 mbdata 2014-01-23 17:00:00 ourcou~ \"POK rally i~       NA        NA\r\n 7 mbdata 2014-01-23 17:00:00 Reggie~ \"great day f~       NA        NA\r\n 8 mbdata 2014-01-23 17:00:00 MindOf~ \"Ugh, these ~       NA        NA\r\n 9 mbdata 2014-01-23 17:00:00 Friend~ \"massive ral~       NA        NA\r\n10 ccdata 2014-01-23 17:00:00 <NA>    \"KEEP THE PE~       NA        NA\r\n# ... with 4,053 more rows, and 2 more variables: location <chr>,\r\n#   clean_message <chr>\r\n\r\n#split into blog and text transcripts of emergency call\r\n\r\n\r\nblog <- filter(raw_text,type=='mbdata')\r\ncall <- filter(raw_text,type=='ccdata') \r\n\r\n\r\n\r\n#2. text mining and word cloud for blog\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(blog$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] Follow us                                                             \r\n[2] Don't miss a moment!  Follow our live coverage of  POK Rally in  Park!\r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 10,max.words=300,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\n2. text mining and word cloud for call\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(call$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] KEEP THE PEACE-CROWD CONTROL/ABILA CITY PARK\r\n[2] TRAFFIC STOP                                \r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,max.words=100,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n#create new column: ID and Time, bin time for every 15 minutes\r\nblog$ID <- seq.int(nrow(blog))\r\nblog$time_bin = cut(blog$`date(yyyyMMddHHmmss)`, breaks=\"60 mins\")\r\nblog$time_bin<-blog$time_bin %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\nblog_topic<-blog%>%\r\n  group_by(time_bin) %>% \r\n  unnest_tokens(word, message) %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n\r\nThe graph below shows the strong correlation from 6pm to 9pm. Overall, the correlation in average is higher than 0.8. It shows the importance of time in the event. We can narrow down the focused time period from 5-9pm to 6-9pm now.\r\n\r\n\r\n#draw a correlation graph to see what are the timing that events are strongly correlated\r\nblog_cors <- blog_topic %>% pairwise_cor(time_bin,word,n,sort = TRUE)\r\n\r\nset.seed(1234)\r\nblog_cors %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"fr\") +\r\n  geom_edge_link(aes(alpha = correlation,width = correlation)) +\r\n  geom_node_point(size = 6,color = \"blue\") +\r\n  geom_node_text(aes(label = name),color = \"red\",repel = TRUE) +\r\n  ggtitle(\"Correlation between time\")+\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nNow we look at the frequent words in each hour and try to extract inccident from it.\r\n\r\n\r\ntf_idf <- blog_topic%>%\r\n  bind_tf_idf(word,time_bin, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\ntf_idf %>%\r\n  group_by(time_bin) %>%\r\n  slice_max(tf_idf,n =10) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder(word,tf_idf)) %>%\r\n  ggplot(aes(tf_idf,word,fill = time_bin)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ time_bin,scales = \"free\",as.table=TRUE) +\r\n  labs(x ='mean tf-idf_score',y= NULL) +\r\n  ggtitle(\"Term Frequency by hour\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nblog_dtm<- blog_topic %>%\r\n  cast_dtm(time_bin,word,n)\r\nLDA <- LDA(blog_dtm, k = 10, control = list(seed = 1234))\r\nap_topics <- tidy(LDA, matrix = \"beta\")\r\nap_topics\r\n\r\n\r\n# A tibble: 31,110 x 3\r\n   topic term     beta\r\n   <int> <chr>   <dbl>\r\n 1     1 the   0.0354 \r\n 2     2 the   0.0429 \r\n 3     3 the   0.00248\r\n 4     4 the   0.0581 \r\n 5     5 the   0.00236\r\n 6     6 the   0.0932 \r\n 7     7 the   0.0196 \r\n 8     8 the   0.0262 \r\n 9     9 the   0.0361 \r\n10    10 the   0.0945 \r\n# ... with 31,100 more rows\r\n\r\n\r\n<div class=\"layout-chunk\" data-layout=\"l-body\">\r\n<div class=\"sourceCode\"><pre class=\"sourceCode r\"><code class=\"sourceCode r\"><span class='cn'>FALSE<\/span>\r\n<\/code><\/pre><\/div>\r\n\r\n[1] FALSE\r\n\r\n<\/div>\r\n\r\n\r\n```{.r .distill-force-highlighting-css}\r\n\r\n\r\n",
    "preview": "posts/2021-07-14-vast-2021-mc-3/vast-2021-mc-3_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-07-25T01:37:03+08:00",
    "input_file": "vast-2021-mc-3.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to BizMakeover 2",
    "description": {},
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T13:21:06+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-06-20-the-sharpe-ratio/",
    "title": "Biz Makeover2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1.0 Critique of Visualisation\r\n1.1 Clarity\r\n1.2 Aesthetic\r\n\r\n2.0 Alternative Design\r\n2.2 Aesthetic\r\n\r\n3.0 Proposed Visualisation\r\n4.0 Step-by-step Guide\r\n5.0 Derived Insights\r\n\r\n1.0 Critique of Visualisation\r\nThe original visualisation is shown below.\r\n\r\n1.1 Clarity\r\nNo title and legend in the graph Without title of graph, readers cannot extract the main objective of the graph. There is no specific illustration about time frame of the data extracted from the graph.\r\nInconsistent axis header Names of both y-axis and x-axis are inconsistent. For instance, there are two headers in x-axis: “NET EXPORTS” and “Exports”. Reader might be confused which value it is.\r\nUnclear of derived values There is no explanation of calculation of net import and net export.\r\nUnclear source of comment There is short paragraph describe trading in Singapore between China and United States in the bottom of picture. Those information cannot be observed from the graph which lacks evidence.\r\n1.2 Aesthetic\r\nToo many colors in the graph There are ten countries in the graph, each of them has one color but they stand for the same meaning: total number of trading volume in the year. Rainbow colors should be avoided.\r\nRepeating monetary units Every amount in the graph has a monetary unit S$ xx Bil in the graph. It was repeated for ten times.\r\nFull opacity There are areas where countries overlap each other, due to full opacity, it is covered and block reader’s from full picture of the graph.\r\nConfusing position of top net importer and top net exporter sign Sign of top net importer and top net exporter are positioned above countries. The shape of the sign is circle which looks like countries whose sign is also circle. Readers might treat them as countries at a glance.\r\n2.0 Alternative Design\r\nThe proposed design is shown in the following.\r\n ## 2.1 Clarity\r\nInteractive panel allows audience to choose countries select and animations allow readers to see how trading volume changes over time.\r\nBox plot is used to show uncertainty distribution variability over time.\r\nOutliers can show clearly what are months trading falls out of normal range and it can be reflected in responding heat map.\r\n2.2 Aesthetic\r\nAnnotations are used to emphases key observations.\r\nBright and contrastive colors are applied so that readers can focus on their interested category.\r\n3.0 Proposed Visualisation\r\nFor a clearer graph, please click the link on Tableau Public here.\r\n4.0 Step-by-step Guide\r\nOpen raw data using excel.\r\n\r\nRemove first 5 rows in both spreadsheet T1 and T2 to ensure header is in first row.\r\n\r\nRemove row 2 to row 8 in both sheet T1 and T2, since those rows are cumulative values for continents.\r\n\r\nDelete row 115 to 128 in sheet T1 as they are commentary. Similarly, repeat that in T2.\r\n\r\nRemove column B to PE since requested period is between January 2011 to December 2020 in both sheets. Similarly, remove data from January 2021 to April 2021 in both sheets.\r\n\r\nInsert a new column in sheet T1, name it as “Type”, fill the rows with “Import”. Repeat the same procedure in sheet T2 except the name is “Export” instead.\r\n\r\nSet range of table to better insert into tableau.\r\n\r\n\r\nSelect variables and type and unpivot rest of columns.\r\n\r\nRename the 3rd and 4th columns into month and trade value, respectively.\r\n\r\nClose and load the table, a new table 1 will appear, rename the sheet as import.\r\n\r\nMultiply trade value by 1000 to reflect real trading amount.\r\n\r\nRepeat step g to k in export sheet as well. Combine these 2 tables into a new excel workbook. Inner join the table by country name and month of the year.\r\n\r\nRename the variables and change datatype of month from string to date.\r\n\r\ncreate new variables Net Import and Net Export using calculation field.\r\n\r\nCreate upper and lower bond for net export.\r\n\r\nDrag lower and upper bound to measure values.\r\n\r\nPut month in column, sum of net export and measure values in rows. Filter by country and measure names.\r\n\r\nAdd average reference line, change the format and add the customized label.\r\n\r\nCreate new variable Outlier and drag it into color pane.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nChart: Net Export time series with outliers is completed shown as such. Repeat the same step to create Net Import chart.\r\n\r\nCreate a new sheet, drag month of year to column and year in row to create a heatmap.\r\n\r\nDrag net export to color and measure names to detail.\r\n\r\nApply filter by country. A heat map is created completely.\r\n\r\nTo show uncertainties of net import over time, box plot is created. Drag year to columns, net export as rows.\r\n\r\nCreate 2 new variables, positive/negative net export using if condition and drag them into tooltip.\r\n\r\nAdd filter by country and measure names as well. Drag month of year and measure names into tooltip.\r\n\r\nEdit tooltip where net export with negative value is blue color while positive one is red.\r\n\r\nAdjust size of circle and reduce opacity.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nBox plot is completed as shown.\r\n\r\nCreate ranking variables using rank_dense for net export and also net import.\r\n\r\nCreate a new sheet, drag country to column and net export to rows.\r\n\r\nApply year, net export and country (to select multiple countries) to filter.\r\n\r\nDrag export trade value, import trade value, net import ranking, net export ranking, net export, positive net export, negative net import, year into detail. Choose net export to color pane.\r\n\r\nClick sort to rank them in descending order.\r\n\r\nEdit tooltip as below.\r\n\r\nThe final main panel is shown.\r\n\r\nOpen a dashboard to drag all these graphs inside, select automatic size.\r\n\r\nSwitch on animation for all sheets.\r\n\r\nShow filter of countries, select worksheet it applies.\r\n\r\nShow filter of year and multiple countries, select single value for first country and period.\r\n\r\n\r\nShow highlight of all field.\r\n\r\nHide time series header for net import.\r\n\r\nThe final dashboard is ready. \r\n5.0 Derived Insights\r\nMainland China: The net export results from January 2011 to December 2018 shows an average performance between $1,221,107,952 and -$38,569,636. There was a sharp decreased in net export in January 2019 which rates about -$1,178,839,000. This sharp fall bounced right back to $1,832,598,000 on the following month, February 2019. Outstanding increase of net export value also shown in August 2019 which performed a highest rate among all the years about $2,077,969,000. \r\nUnited States of America:  Back in January 2011 to January 2020, there is an underperformance net exports shown in the graph which range between -$511,229,502 to -$1,996,866,464. In the year of 2020, U.S net export values increased from -$568.861,000 to $2 billion. This outstanding performance started from January 2020 to July 2020. \r\nThailand:  Starting from January 2011, Thailand has an average performance of their net export values till September 2019. The performance increased to $1.4 billion from September 2019 to March 2020. However, the following month in April 2020, there was a noticeable drop to -$5.6 million.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T16:55:11+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
