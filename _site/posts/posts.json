[
  {
    "path": "posts/2021-07-14-vast-2021-mc-3/",
    "title": "Vast 2021 MC 3",
    "description": "Visual analytics assignment: mini challenge 3",
    "author": [
      {
        "name": "Li Shuxian",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\nimport packages\r\n\r\n\r\n\r\nimport data\r\n\r\n\r\nsegment1 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1700-1830.csv')\r\nsegment2 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1831-2000.csv')\r\nsegment3 <-read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-2001-2131.csv')\r\n\r\n\r\n\r\ncombine all these 3 segments data together\r\n\r\n\r\nraw_text <- rbind(segment1,segment2,segment3)\r\n\r\n\r\n\r\nWrangling time\r\n\r\n\r\nraw_text$`date(yyyyMMddHHmmss)`<-ymd_hms(raw_text$`date(yyyyMMddHHmmss)`)\r\n\r\n\r\n\r\n1.1 clean data,raw_text\r\n\r\n\r\nraw_text$clean_message <-raw_text$message%>%\r\n  tolower()%>%#change all messages to lowercase\r\n  replace_contraction()%>%#remove short form\r\n  replace_word_elongation()%>% #remove the same letter appears unnecessarily, eg.'loooook' to 'look'\r\n  str_squish()%>% #re3moves space from start and end of string\r\n  lemmatize_strings()%>%#perform lemmatization\r\n  removeWords(stopwords('english'))#%>%#remove stopwords\r\n\r\n\r\n\r\n1.2 clean data,remove keywords in the message - these messages are identified as junk messages\r\n\r\n\r\nraw_text$clean_message <-raw_text$message %>% \r\n  #remove rt @ in the message, replace with\"\"\r\n  str_replace_all(\"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  str_replace_all(\"rt @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove @ in the message, replace with\"\"\r\n  str_replace_all(\"@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove # in the message, replace with\"\"\r\n  str_replace_all(\"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove stop word:the/The/to/ of / is/ in /you /and/ have/ at /are /for/ on/your/ it/ that /be with /more \r\n  gsub(pattern ='The',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='the',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='to',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='of',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='is',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='in',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='you',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='your',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='and',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='have',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='at',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='are',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='for',replacement = \"\",raw_text$message)%>%\r\n   gsub(pattern ='on',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='it',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='that',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='be',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='with',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='more',replacement = \"\",raw_text$message)\r\n\r\n\r\n\r\n1.3 stemming message\r\n\r\n\r\npr_stem_words(raw_text,clean_message,language = \"english\")\r\n\r\n\r\n# A tibble: 4,063 x 8\r\n   type   `date(yyyyMMddHHmm~ author  message       latitude longitude\r\n * <chr>  <dttm>              <chr>   <chr>            <dbl>     <dbl>\r\n 1 mbdata 2014-01-23 17:00:00 POK     \"Follow us @~       NA        NA\r\n 2 mbdata 2014-01-23 17:00:00 maha_H~ \"Don't miss ~       NA        NA\r\n 3 mbdata 2014-01-23 17:00:00 Viktor~ \"Come join u~       NA        NA\r\n 4 mbdata 2014-01-23 17:00:00 Kronos~ \"POK rally t~       NA        NA\r\n 5 mbdata 2014-01-23 17:00:00 AbilaP~ \"POK rally s~       NA        NA\r\n 6 mbdata 2014-01-23 17:00:00 ourcou~ \"POK rally i~       NA        NA\r\n 7 mbdata 2014-01-23 17:00:00 Reggie~ \"great day f~       NA        NA\r\n 8 mbdata 2014-01-23 17:00:00 MindOf~ \"Ugh, these ~       NA        NA\r\n 9 mbdata 2014-01-23 17:00:00 Friend~ \"massive ral~       NA        NA\r\n10 ccdata 2014-01-23 17:00:00 <NA>    \"KEEP THE PE~       NA        NA\r\n# ... with 4,053 more rows, and 2 more variables: location <chr>,\r\n#   clean_message <chr>\r\n\r\n1.4 split into blog and text transcripts of emergency call\r\n\r\n\r\nblog <- filter(raw_text,type=='mbdata')\r\ncall <- filter(raw_text,type=='ccdata') \r\n\r\n\r\n\r\n1.5 data explore: text mining and word cloud for blog\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(blog$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] Follow us                                                     \r\n[2] D't ms a moment!  Follow our live coverage   POK Rally   Park!\r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 10,max.words=300,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nThere are some stopword should be removed seeing from the word cloud graph. They are: to, of, a, i,s in, you, and, have, at, are, for, on, i, your, it, that, be, with, more.\r\n1.6 data explore: text mining and word cloud for call\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(call$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] KEEP THE PEACE-CROWD CONTROL/ABILA CITY PARK\r\n[2] TRAFFIC STOP                                \r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,max.words=100,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nQuestion 1. Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.\r\n\r\n\r\n#create new column: ID and Time, bin time for every hour for blog\r\nblog$ID <- seq.int(nrow(blog))\r\nblog$time_bin = cut(blog$`date(yyyyMMddHHmmss)`, breaks=\"60 mins\")\r\nblog$time_bin<-blog$time_bin %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\nblog_topic<-blog%>%\r\n  group_by(time_bin) %>% \r\n  unnest_tokens(word, clean_message) %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n#draw a correlation graph to see what are the timing that events are strongly correlated\r\nblog_cors <- blog_topic %>% pairwise_cor(time_bin,word,n,sort = TRUE)\r\n\r\nset.seed(1234)\r\nblog_cors %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"fr\") +\r\n  geom_edge_link(aes(alpha = correlation,width = correlation)) +\r\n  geom_node_point(size = 6,color = \"blue\") +\r\n  geom_node_text(aes(label = name),color = \"red\",repel = TRUE) +\r\n  ggtitle(\"Correlation between time\")+\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nThe graph above shows the strong correlation from 6pm to 9pm. Overall, the correlation in average is higher than 0.8. It shows the importance of time in the event. We can narrow down the focused time period from 5-9pm to 6-9pm now.\r\nNow we look at the frequent words in each hour and try to extract inccident from it.\r\n\r\n\r\ntf_idf <- blog_topic%>%\r\n  bind_tf_idf(word,time_bin, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\ntf_idf %>%\r\n  group_by(time_bin) %>%\r\n  slice_max(tf_idf,n =10) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder(word,tf_idf)) %>%\r\n  ggplot(aes(tf_idf,word,fill = time_bin)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ time_bin,scales = \"free\",as.table=TRUE) +\r\n  labs(x ='mean tf-idf_score',y= NULL) +\r\n  ggtitle(\"Blog Term Frequency by hour\")\r\n\r\n\r\n\r\n\r\nFrom the TF bar chart above,there are a few events we can make a guess.\r\nEvent 1,there was a fire happened at appartment: dancing dolphin at 6 pm.Neighborhood evacuations started around 7pm. Firefighter came around 8pm and was injured. The fire caused explosion at 9pm.Suspects of arsonist were arrested.Event 2,there was a shot fired by officer at 7 pm.Event 3, probably there was a car/van accident at 7pm.\r\nword cloud by hour for blog\r\n\r\n\r\nset.seed(1234)\r\nblog_topic %>%\r\n  group_by(time_bin) %>% \r\n  slice_max(order_by = n, n = 100) %>% \r\nggplot(aes(label = word,size = n,col = as.character(n))) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  ggtitle(\"Blog:Word Cloud by hour\")+\r\n  facet_wrap(~time_bin)\r\n\r\n\r\n\r\n\r\nFrom the word cloud above, words like “the”,“and”,“a”,“is” are ignored. All these keywords are supported evidence for those events mentioned previously. 1.“Pokrally” is certainly a important word over the hours. Given the background information, it is a gathering from organization Protectors of Kronos.This is one more event detected from word cloud. 2.“Kronosstar” and “abilapost” are highly frequent term. It is an author name which means it is either he post a lot or he is an influencer and being retweet a lot.\r\nExtract per-topic-per-word probabilities β/beta from the model. The higher the value, the more significant the words are to the topic.Then run a topic modeling LDA to extract keywords.Use gamma to assign each document to a topic.\r\n\r\n\r\nblogDTM <-blog_topic%>%cast_dtm(time_bin,word,n)\r\nblogLDA <-LDA(blogDTM, k = 20, control = list(seed = 1234))\r\ntopicProb <- tidy(blogLDA, matrix = \"beta\")\r\ntopicProb\r\n\r\n\r\n# A tibble: 55,080 x 3\r\n   topic term     beta\r\n   <int> <chr>   <dbl>\r\n 1     1 a     0.0268 \r\n 2     2 a     0.0200 \r\n 3     3 a     0.0247 \r\n 4     4 a     0.00110\r\n 5     5 a     0.0187 \r\n 6     6 a     0.00411\r\n 7     7 a     0.0196 \r\n 8     8 a     0.0304 \r\n 9     9 a     0.0273 \r\n10    10 a     0.00303\r\n# ... with 55,070 more rows\r\n\r\n\r\n\r\nblogGamma<-tidy(blogLDA, matrix = \"gamma\")%>%group_by(document)\r\nblogGamma %>%\r\n  mutate(title = reorder(document, gamma * topic)) %>%\r\n  ggplot(aes(factor(topic), gamma)) +\r\n  geom_boxplot() +\r\n  ggtitle(\"Blog:Topic Distribution over time\")+\r\n  facet_wrap(~ title)\r\n\r\n\r\n\r\n\r\nThis graph help us to determine the influence of each event.It is determined significant topic when their gamma score is greater than 0.3. At 5 pm, topic 6,9,19 are the ones. Extract useful words from topic them,we can determine it is pokrally (start of event 5).At 6pm, topic 13 shoots to the top and reaches peak among the 4 hours. The word “abila” raised the attention. They are participants for event 2 (fire).At 6pm, topic 3,8,10 are hot topics. Key words are “pokrally” and “kronosstar”.At 7pm, topic 1 and 13 are trendy topics which has the same content as previous.\r\n\r\n\r\nTopic <- topicProb %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n =5) %>% \r\n  ungroup() %>%\r\n  arrange(topic, -beta)\r\n\r\n\r\n\r\n\r\n\r\nTopic %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  scale_y_reordered()+\r\n  ggtitle(\"Blog:Topic Modeling\")+\r\n  warnings()\r\n\r\n\r\n\r\n\r\nCrime: appear in topic 1,2,3,5,15,16. Traffic: appear in topic 1,2,4,8,9,10,11,12,14,17,18,19,20. There are multiple words appear in multiple topics. It shows the importance to plot the graph by time. Between 6-7pm, topic 13 shows in the peak of the graph. Key words about the car accidents are extracted: a black van vehicle hit someone/a car.\r\nRepeat the same step for calls.\r\n\r\n\r\n#create new column: ID and Time, bin time for every hour for call\r\ncall$ID <- seq.int(nrow(call))\r\ncall$time_bin = cut(call$`date(yyyyMMddHHmmss)`, breaks=\"60 mins\")\r\ncall$time_bin<-call$time_bin %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\ncall_topic<-call%>%\r\n  group_by(time_bin) %>% \r\n  unnest_tokens(word, message) %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n\r\nword cloud by hour for call\r\n\r\n\r\nset.seed(1234)\r\ncall_topic %>%\r\n  group_by(time_bin) %>% \r\n  slice_max(order_by = n, n = 20) %>% \r\nggplot(aes(label = word,size = n,col = as.character(n))) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  ggtitle(\"Call:Word Cloud by hour\")+\r\n  facet_wrap(~time_bin)\r\n\r\n\r\n\r\n\r\nFrom word cloud of emergency call, the word “stop” appeared frequently. It is assumed people who stayed nearby dancing dolphin might call fire fighting hotline to stop the fires.\r\n\r\n\r\ncallDTM <-call_topic%>%cast_dtm(time_bin,word,n)\r\ncallLDA <-LDA(callDTM, k = 20, control = list(seed = 1234))\r\ntopicProb <- tidy(callLDA, matrix = \"beta\")\r\ntopicProb\r\n\r\n\r\n# A tibble: 2,100 x 3\r\n   topic term     beta\r\n   <int> <chr>   <dbl>\r\n 1     1 stop  0.138  \r\n 2     2 stop  0.0953 \r\n 3     3 stop  0.208  \r\n 4     4 stop  0.0863 \r\n 5     5 stop  0.110  \r\n 6     6 stop  0.00461\r\n 7     7 stop  0.0891 \r\n 8     8 stop  0.0141 \r\n 9     9 stop  0.188  \r\n10    10 stop  0.0210 \r\n# ... with 2,090 more rows\r\n\r\ncallGamma<-tidy(callLDA, matrix = \"gamma\")%>%group_by(document)\r\ncallGamma %>%\r\n  mutate(title = reorder(document, gamma * topic)) %>%\r\n  ggplot(aes(factor(topic), gamma)) +\r\n  geom_boxplot() +\r\n  ggtitle(\"Call:Topic Distribution over time\")+\r\n  facet_wrap(~ title)\r\n\r\n\r\n\r\n\r\nTo conclude insights from question 1, there are 4 events found: pok rally starts at 5pm. Fire started between 6 to 7pm at dancing dolphin. After 7pm, evacuations started as the fire got worst. Even the fire fighter injured. Meanwhile, a black van hit a car caused accident.Lastly, the fire caused explosion and suspects for the fire was arrested.\r\nQuestion 2:Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.\r\nZoom in to analyze the changes in the minute unit instead of hour.\r\n\r\n\r\nblog$mins = cut(blog$`date(yyyyMMddHHmmss)`, breaks=\"10 min\")\r\nblog$mins<-blog$mins %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\n\r\n\r\n\r\n\r\ncount<-blog%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n())\r\ndatatable(count,class='mins',style = \"default\", width =NULL, height = NULL,\r\n          elementId = NULL,colnames=c('Time', 'No of Blog Posted'))\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\"],[\" 17:00:00\",\" 17:10:00\",\" 17:20:00\",\" 17:30:00\",\" 17:40:00\",\" 17:50:00\",\" 18:00:00\",\" 18:10:00\",\" 18:20:00\",\" 18:30:00\",\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:00:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"],[92,154,171,100,84,88,96,98,78,89,265,186,156,130,186,126,418,145,182,171,154,160,88,91,93,95,148,28]],\"container\":\"<table class=\\\"mins\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Time<\\/th>\\n      <th>No of Blog Posted<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nNumber of blog posted over time is shown above. Intensive discussion happened in 5:10-5:20 pm. It increased rapidly in 6:40 - 7:00 which is the time fire started.7:40pm, blog posted exceed 400 and it is the highest among all.\r\n\r\n\r\ncall$mins = cut(call$`date(yyyyMMddHHmmss)`, breaks=\"10 min\")\r\ncall$mins<-call$mins %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\n\r\n\r\n\r\n\r\ncount<-call%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n())\r\ndatatable(count,class='mins',style = \"default\", width =NULL, height = NULL,\r\n          elementId = NULL,colnames=c('Time', 'No of Emergency Call'))\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\"],[\" 17:00:00\",\" 17:10:00\",\" 17:20:00\",\" 17:30:00\",\" 17:40:00\",\" 17:50:00\",\" 18:00:00\",\" 18:10:00\",\" 18:20:00\",\" 18:30:00\",\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:00:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"],[3,11,13,7,8,4,6,6,3,9,19,4,4,6,12,8,26,4,6,5,5,4,3,2,2,5,5,1]],\"container\":\"<table class=\\\"mins\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Time<\\/th>\\n      <th>No of Emergency Call<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nRank number of calls in descending order, it is found that most of the calls made around 7:40pm. This is the timing where gunshot or the car accident happened.\r\nTo continue,I would like to explore distraction impact of these events from pok rally. These selected authors are members from POK such as POK leader Sylvia.\r\n\r\n\r\nPOKrally<-blog%>%filter(str_detect(author,'KronosStar')|\r\n                              str_detect(author,'POK')|\r\n                              str_detect(author,'AbilaPost')|    \r\n                              str_detect(author,'FriendsOfKronos'))%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n()) \r\nplot_ly(data = POKrally,\r\n        x = ~mins,\r\n        y = ~number,\r\n        marker = list(color = \"orange\"),\r\n        colors = \"black\")%>%\r\n  layout(title = 'Pok Park Rally Event')\r\n\r\n\r\n\r\n{\"x\":{\"visdat\":{\"22206798788f\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"22206798788f\",\"attrs\":{\"22206798788f\":{\"x\":{},\"y\":{},\"marker\":{\"color\":\"orange\"},\"colors\":\"black\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Pok Park Rally Event\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"mins\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\" 17:00:00\",\" 17:10:00\",\" 17:20:00\",\" 17:30:00\",\" 17:40:00\",\" 17:50:00\",\" 18:00:00\",\" 18:10:00\",\" 18:30:00\",\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"]},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"number\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[\" 17:00:00\",\" 17:10:00\",\" 17:20:00\",\" 17:30:00\",\" 17:40:00\",\" 17:50:00\",\" 18:00:00\",\" 18:10:00\",\" 18:30:00\",\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"],\"y\":[8,22,13,11,8,5,8,9,5,28,16,9,5,7,5,26,9,14,14,12,13,2,2,3,11,2],\"marker\":{\"color\":\"orange\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"type\":\"bar\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\nFIRE<-blog%>%filter(str_detect(message,'dancingdolphin')|\r\n                              str_detect(message,'dancing dolphin')|\r\n                              str_detect(message,'DancingDolphin')|    \r\n                              str_detect(message,'Dancing Dolphin'))%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n()) \r\nplot_ly(data = FIRE,\r\n        x = ~mins,\r\n        y = ~number,\r\n        marker = list(color = \"black\"),\r\n        colors = \"dark green\")%>%\r\n  layout(title = 'Fire at Dancing Dolphin Event')\r\n\r\n\r\n\r\n{\"x\":{\"visdat\":{\"22201ebdae0\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"22201ebdae0\",\"attrs\":{\"22201ebdae0\":{\"x\":{},\"y\":{},\"marker\":{\"color\":\"black\"},\"colors\":\"dark green\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Fire at Dancing Dolphin Event\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"mins\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"]},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"number\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"],\"y\":[63,21,16,12,19,12,6,2,2,5,1,23,2,4,3,15,15],\"marker\":{\"color\":\"black\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"type\":\"bar\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\nGun<-blog%>%filter(str_detect(message,'gun fire')|\r\n                              str_detect(message,'gunfire')|\r\n                              str_detect(message,'Gun Fire')|    \r\n                              str_detect(message,'GUNFIRE'))%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n()) \r\nplot_ly(data = FIRE,\r\n        x = ~mins,\r\n        y = ~number,\r\n        colors = \"dark ORANGE\")%>%\r\n  layout(title = 'Gunfire Event')\r\n\r\n\r\n\r\n{\"x\":{\"visdat\":{\"222067d71783\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"222067d71783\",\"attrs\":{\"222067d71783\":{\"x\":{},\"y\":{},\"colors\":\"dark ORANGE\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Gunfire Event\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"mins\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"]},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"number\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[\" 18:40:00\",\" 18:50:00\",\" 19:00:00\",\" 19:10:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:10:00\",\" 21:20:00\",\" 21:30:00\"],\"y\":[63,21,16,12,19,12,6,2,2,5,1,23,2,4,3,15,15],\"type\":\"bar\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\nVAN<-blog%>%filter(str_detect(message,'VAN')|str_detect(message,'van'))%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n()) \r\nplot_ly(data = VAN,\r\n        x = ~mins,\r\n        y = ~number,\r\n        marker = list(color = \"green\"),\r\n        colors = \"dark green\")%>%\r\n  layout(title = 'Black Van Accident Event')\r\n\r\n\r\n\r\n{\"x\":{\"visdat\":{\"2220e87d87\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"2220e87d87\",\"attrs\":{\"2220e87d87\":{\"x\":{},\"y\":{},\"marker\":{\"color\":\"green\"},\"colors\":\"dark green\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"title\":\"Black Van Accident Event\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"mins\",\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[\" 17:10:00\",\" 17:30:00\",\" 17:50:00\",\" 18:30:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:00:00\",\" 21:10:00\",\" 21:20:00\"]},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"number\"},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[\" 17:10:00\",\" 17:30:00\",\" 17:50:00\",\" 18:30:00\",\" 19:20:00\",\" 19:30:00\",\" 19:40:00\",\" 19:50:00\",\" 20:00:00\",\" 20:10:00\",\" 20:20:00\",\" 20:30:00\",\" 20:40:00\",\" 20:50:00\",\" 21:00:00\",\" 21:10:00\",\" 21:20:00\"],\"y\":[1,1,1,1,8,1,32,7,8,12,16,7,4,1,13,11,2],\"marker\":{\"color\":\"green\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"type\":\"bar\",\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\n The image above compares number of blog posted change over time. Both gunfire and fire event has more attention than others. Clearly netizen are very active around 6:40 and 7:40 pm.\r\nQuestion 3.If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively?\r\nIf I were able to send a team of first responders to any single place, I would send them to the dolphin dancing apartment to stop fire before it gets bigger. Even we can catch fire starts to stop the fire at the first place.\r\n4. Literature_Review\r\n This figure from IIIT,2014 inspired me to seek for impact of several incident from the event perspective. (https://www.cs.umd.edu/hcil/varepository/VAST%20Challenge%202014/challenges/MC3%20-%20Real-Time,%20Streaming%20Social%20Media/entries/International%20Institute%20of%20Information%20and%20Technology%20-%20Hyderabad/)\r\n Moreover,this timeline graph from TJU 2014 reminds me the importance of timeframe. It is important to divide events and have a clear timeline of things happened.\r\n(http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC3%20-%20Real-Time,%20Streaming%20Social%20Media/entries/Tianjin%20University/)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-14-vast-2021-mc-3/vast-2021-mc-3_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-07-26T14:20:24+08:00",
    "input_file": "vast-2021-mc-3.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to BizMakeover 2",
    "description": {},
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T13:21:06+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-06-20-the-sharpe-ratio/",
    "title": "Biz Makeover2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1.0 Critique of Visualisation\r\n1.1 Clarity\r\n1.2 Aesthetic\r\n\r\n2.0 Alternative Design\r\n2.2 Aesthetic\r\n\r\n3.0 Proposed Visualisation\r\n4.0 Step-by-step Guide\r\n5.0 Derived Insights\r\n\r\n1.0 Critique of Visualisation\r\nThe original visualisation is shown below.\r\n\r\n1.1 Clarity\r\nNo title and legend in the graph Without title of graph, readers cannot extract the main objective of the graph. There is no specific illustration about time frame of the data extracted from the graph.\r\nInconsistent axis header Names of both y-axis and x-axis are inconsistent. For instance, there are two headers in x-axis: “NET EXPORTS” and “Exports”. Reader might be confused which value it is.\r\nUnclear of derived values There is no explanation of calculation of net import and net export.\r\nUnclear source of comment There is short paragraph describe trading in Singapore between China and United States in the bottom of picture. Those information cannot be observed from the graph which lacks evidence.\r\n1.2 Aesthetic\r\nToo many colors in the graph There are ten countries in the graph, each of them has one color but they stand for the same meaning: total number of trading volume in the year. Rainbow colors should be avoided.\r\nRepeating monetary units Every amount in the graph has a monetary unit S$ xx Bil in the graph. It was repeated for ten times.\r\nFull opacity There are areas where countries overlap each other, due to full opacity, it is covered and block reader’s from full picture of the graph.\r\nConfusing position of top net importer and top net exporter sign Sign of top net importer and top net exporter are positioned above countries. The shape of the sign is circle which looks like countries whose sign is also circle. Readers might treat them as countries at a glance.\r\n2.0 Alternative Design\r\nThe proposed design is shown in the following.\r\n ## 2.1 Clarity\r\nInteractive panel allows audience to choose countries select and animations allow readers to see how trading volume changes over time.\r\nBox plot is used to show uncertainty distribution variability over time.\r\nOutliers can show clearly what are months trading falls out of normal range and it can be reflected in responding heat map.\r\n2.2 Aesthetic\r\nAnnotations are used to emphases key observations.\r\nBright and contrastive colors are applied so that readers can focus on their interested category.\r\n3.0 Proposed Visualisation\r\nFor a clearer graph, please click the link on Tableau Public here.\r\n4.0 Step-by-step Guide\r\nOpen raw data using excel.\r\n\r\nRemove first 5 rows in both spreadsheet T1 and T2 to ensure header is in first row.\r\n\r\nRemove row 2 to row 8 in both sheet T1 and T2, since those rows are cumulative values for continents.\r\n\r\nDelete row 115 to 128 in sheet T1 as they are commentary. Similarly, repeat that in T2.\r\n\r\nRemove column B to PE since requested period is between January 2011 to December 2020 in both sheets. Similarly, remove data from January 2021 to April 2021 in both sheets.\r\n\r\nInsert a new column in sheet T1, name it as “Type”, fill the rows with “Import”. Repeat the same procedure in sheet T2 except the name is “Export” instead.\r\n\r\nSet range of table to better insert into tableau.\r\n\r\n\r\nSelect variables and type and unpivot rest of columns.\r\n\r\nRename the 3rd and 4th columns into month and trade value, respectively.\r\n\r\nClose and load the table, a new table 1 will appear, rename the sheet as import.\r\n\r\nMultiply trade value by 1000 to reflect real trading amount.\r\n\r\nRepeat step g to k in export sheet as well. Combine these 2 tables into a new excel workbook. Inner join the table by country name and month of the year.\r\n\r\nRename the variables and change datatype of month from string to date.\r\n\r\ncreate new variables Net Import and Net Export using calculation field.\r\n\r\nCreate upper and lower bond for net export.\r\n\r\nDrag lower and upper bound to measure values.\r\n\r\nPut month in column, sum of net export and measure values in rows. Filter by country and measure names.\r\n\r\nAdd average reference line, change the format and add the customized label.\r\n\r\nCreate new variable Outlier and drag it into color pane.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nChart: Net Export time series with outliers is completed shown as such. Repeat the same step to create Net Import chart.\r\n\r\nCreate a new sheet, drag month of year to column and year in row to create a heatmap.\r\n\r\nDrag net export to color and measure names to detail.\r\n\r\nApply filter by country. A heat map is created completely.\r\n\r\nTo show uncertainties of net import over time, box plot is created. Drag year to columns, net export as rows.\r\n\r\nCreate 2 new variables, positive/negative net export using if condition and drag them into tooltip.\r\n\r\nAdd filter by country and measure names as well. Drag month of year and measure names into tooltip.\r\n\r\nEdit tooltip where net export with negative value is blue color while positive one is red.\r\n\r\nAdjust size of circle and reduce opacity.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nBox plot is completed as shown.\r\n\r\nCreate ranking variables using rank_dense for net export and also net import.\r\n\r\nCreate a new sheet, drag country to column and net export to rows.\r\n\r\nApply year, net export and country (to select multiple countries) to filter.\r\n\r\nDrag export trade value, import trade value, net import ranking, net export ranking, net export, positive net export, negative net import, year into detail. Choose net export to color pane.\r\n\r\nClick sort to rank them in descending order.\r\n\r\nEdit tooltip as below.\r\n\r\nThe final main panel is shown.\r\n\r\nOpen a dashboard to drag all these graphs inside, select automatic size.\r\n\r\nSwitch on animation for all sheets.\r\n\r\nShow filter of countries, select worksheet it applies.\r\n\r\nShow filter of year and multiple countries, select single value for first country and period.\r\n\r\n\r\nShow highlight of all field.\r\n\r\nHide time series header for net import.\r\n\r\nThe final dashboard is ready. \r\n5.0 Derived Insights\r\nMainland China: The net export results from January 2011 to December 2018 shows an average performance between $1,221,107,952 and -$38,569,636. There was a sharp decreased in net export in January 2019 which rates about -$1,178,839,000. This sharp fall bounced right back to $1,832,598,000 on the following month, February 2019. Outstanding increase of net export value also shown in August 2019 which performed a highest rate among all the years about $2,077,969,000. \r\nUnited States of America:  Back in January 2011 to January 2020, there is an underperformance net exports shown in the graph which range between -$511,229,502 to -$1,996,866,464. In the year of 2020, U.S net export values increased from -$568.861,000 to $2 billion. This outstanding performance started from January 2020 to July 2020. \r\nThailand:  Starting from January 2011, Thailand has an average performance of their net export values till September 2019. The performance increased to $1.4 billion from September 2019 to March 2020. However, the following month in April 2020, there was a noticeable drop to -$5.6 million.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T16:55:11+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
