[
  {
    "path": "posts/2021-07-14-vast-2021-mc-3/",
    "title": "Vast 2021 MC 3",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Li Shuxian",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\nimport packages\r\n\r\n\r\n\r\nimport data\r\n\r\n\r\nsegment1 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1700-1830.csv')\r\nsegment2 <- read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-1831-2000.csv')\r\nsegment3 <-read_csv('H:/jovina7/BizMakeover2/_posts/2021-07-14-vast-2021-mc-3/data/csv-2001-2131.csv')\r\n\r\n\r\n\r\ncombine all these 3 segments data together\r\n\r\n\r\nraw_text <- rbind(segment1,segment2,segment3)\r\n\r\n\r\n\r\nWrangling time\r\n\r\n\r\nraw_text$`date(yyyyMMddHHmmss)`<-ymd_hms(raw_text$`date(yyyyMMddHHmmss)`)\r\n\r\n\r\n\r\n1.1 clean data,raw_text\r\n\r\n\r\nraw_text$clean_message <-raw_text$message%>%\r\n  tolower()%>%#change all messages to lowercase\r\n  replace_contraction()%>%#remove short form\r\n  replace_word_elongation()%>% #remove the same letter appears unnecessarily, eg.'loooook' to 'look'\r\n  str_squish()%>% #re3moves space from start and end of string\r\n  lemmatize_strings()%>%#perform lemmatization\r\n  removeWords(stopwords('english'))#%>%#remove stopwords\r\n\r\n\r\n\r\n1.2 clean data,remove keywords in the message - these messages are identified as junk messages\r\n\r\n\r\nraw_text$clean_message <-raw_text$message %>% \r\n  #remove rt @ in the message, replace with\"\"\r\n  str_replace_all(\"RT @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  str_replace_all(\"rt @([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove @ in the message, replace with\"\"\r\n  str_replace_all(\"@([A-Za-z]+[A-Za-z0-9_-]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove # in the message, replace with\"\"\r\n  str_replace_all(\"#([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)\",\"\")%>%\r\n  #remove stop word:the/The/to/ of / is/ in /you /and/ have/ at /are /for/ on/your/ it/ that /be with /more \r\n  gsub(pattern ='The',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='the',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='to',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='of',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='is',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='in',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='you',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='your',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='and',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='have',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='at',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='are',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='for',replacement = \"\",raw_text$message)%>%\r\n   gsub(pattern ='on',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='it',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='that',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='be',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='with',replacement = \"\",raw_text$message)%>%\r\n  gsub(pattern ='more',replacement = \"\",raw_text$message)\r\n\r\n\r\n\r\n1.3 stemming message\r\n\r\n\r\npr_stem_words(raw_text,clean_message,language = \"english\")\r\n\r\n\r\n# A tibble: 4,063 x 8\r\n   type   `date(yyyyMMddHHmm~ author  message       latitude longitude\r\n * <chr>  <dttm>              <chr>   <chr>            <dbl>     <dbl>\r\n 1 mbdata 2014-01-23 17:00:00 POK     \"Follow us @~       NA        NA\r\n 2 mbdata 2014-01-23 17:00:00 maha_H~ \"Don't miss ~       NA        NA\r\n 3 mbdata 2014-01-23 17:00:00 Viktor~ \"Come join u~       NA        NA\r\n 4 mbdata 2014-01-23 17:00:00 Kronos~ \"POK rally t~       NA        NA\r\n 5 mbdata 2014-01-23 17:00:00 AbilaP~ \"POK rally s~       NA        NA\r\n 6 mbdata 2014-01-23 17:00:00 ourcou~ \"POK rally i~       NA        NA\r\n 7 mbdata 2014-01-23 17:00:00 Reggie~ \"great day f~       NA        NA\r\n 8 mbdata 2014-01-23 17:00:00 MindOf~ \"Ugh, these ~       NA        NA\r\n 9 mbdata 2014-01-23 17:00:00 Friend~ \"massive ral~       NA        NA\r\n10 ccdata 2014-01-23 17:00:00 <NA>    \"KEEP THE PE~       NA        NA\r\n# ... with 4,053 more rows, and 2 more variables: location <chr>,\r\n#   clean_message <chr>\r\n\r\n1.4 split into blog and text transcripts of emergency call\r\n\r\n\r\nblog <- filter(raw_text,type=='mbdata')\r\ncall <- filter(raw_text,type=='ccdata') \r\n\r\n\r\n\r\n1.5 data explore: text mining and word cloud for blog\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(blog$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] Follow us                                                     \r\n[2] D't ms a moment!  Follow our live coverage   POK Rally   Park!\r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 10,max.words=300,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nThere are some stopword should be removed seeing from the word cloud graph. They are: to, of, a, i,s in, you, and, have, at, are, for, on, i, your, it, that, be, with, more.\r\n1.6 data explore: text mining and word cloud for call\r\n\r\n\r\n#text transform: convert dataframe to corpus\r\ndocs <- Corpus(VectorSource(as.character(call$clean_message)))\r\ninspect(docs[1:2])\r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 2\r\n\r\n[1] KEEP THE PEACE-CROWD CONTROL/ABILA CITY PARK\r\n[2] TRAFFIC STOP                                \r\n\r\n#build a term-document matrix\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\n# words and frequency dataframe\r\ndf <- data.frame(word = names(words),freq=words)\r\n#generate word cloud\r\nset.seed(1234)\r\nwordcloud(words = df$word, freq = df$freq, min.freq = 5,max.words=100,\r\n          random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nQuestion 1. Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? Please limit your answer to 8 images and 500 words.\r\n\r\n\r\n#create new column: ID and Time, bin time for every hour for blog\r\nblog$ID <- seq.int(nrow(blog))\r\nblog$time_bin = cut(blog$`date(yyyyMMddHHmmss)`, breaks=\"60 mins\")\r\nblog$time_bin<-blog$time_bin %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\nblog_topic<-blog%>%\r\n  group_by(time_bin) %>% \r\n  unnest_tokens(word, clean_message) %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n#draw a correlation graph to see what are the timing that events are strongly correlated\r\nblog_cors <- blog_topic %>% pairwise_cor(time_bin,word,n,sort = TRUE)\r\n\r\nset.seed(1234)\r\nblog_cors %>%\r\n  graph_from_data_frame() %>%\r\n  ggraph(layout = \"fr\") +\r\n  geom_edge_link(aes(alpha = correlation,width = correlation)) +\r\n  geom_node_point(size = 6,color = \"blue\") +\r\n  geom_node_text(aes(label = name),color = \"red\",repel = TRUE) +\r\n  ggtitle(\"Correlation between time\")+\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nThe graph above shows the strong correlation from 6pm to 9pm. Overall, the correlation in average is higher than 0.8. It shows the importance of time in the event. We can narrow down the focused time period from 5-9pm to 6-9pm now.\r\nNow we look at the frequent words in each hour and try to extract inccident from it.\r\n\r\n\r\ntf_idf <- blog_topic%>%\r\n  bind_tf_idf(word,time_bin, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\ntf_idf %>%\r\n  group_by(time_bin) %>%\r\n  slice_max(tf_idf,n =10) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder(word,tf_idf)) %>%\r\n  ggplot(aes(tf_idf,word,fill = time_bin)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ time_bin,scales = \"free\",as.table=TRUE) +\r\n  labs(x ='mean tf-idf_score',y= NULL) +\r\n  ggtitle(\"Blog Term Frequency by hour\")\r\n\r\n\r\n\r\n\r\nFrom the TF bar chart above,there are a few events we can make a guess.\r\nEvent 1,there was a fire happened at appartment: dancing dolphin at 6 pm.Neighborhood evacuations started around 7pm. Firefighter came around 8pm and was injured. The fire caused explosion at 9pm.Suspects of arsonist were arrested.Event 2,there was a shot fired by officer at 7 pm.Event 3, probably there was a car/van accident at 7pm.\r\nword cloud by hour for blog\r\n\r\n\r\nset.seed(1234)\r\nblog_topic %>%\r\n  group_by(time_bin) %>% \r\n  slice_max(order_by = n, n = 100) %>% \r\nggplot(aes(label = word,size = n,col = as.character(n))) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  ggtitle(\"Blog:Word Cloud by hour\")+\r\n  facet_wrap(~time_bin)\r\n\r\n\r\n\r\n\r\nFrom the word cloud above, words like “the”,“and”,“a”,“is” are ignored.\r\nAll these keywords are supported evidence for those events mentioned previously.\r\n1.“Pokrally” is certainly a important word over the hours. Given the background information, it is a gathering from organization Protectors of Kronos.This is one more event detected from word cloud.\r\n2.“Kronosstar” and “abilapost” are highly frequent term. It is an author name which means it is either he post a lot or he is an influencer and being retweet a lot.\r\nExtract per-topic-per-word probabilities β/beta from the model. The higher the value, the more significant the words are to the topic.Then run a topic modeling LDA to extract keywords.Use gamma to assign each document to a topic.\r\n\r\n\r\nblogDTM <-blog_topic%>%cast_dtm(time_bin,word,n)\r\nblogLDA <-LDA(blogDTM, k = 20, control = list(seed = 1234))\r\ntopicProb <- tidy(blogLDA, matrix = \"beta\")\r\ntopicProb\r\n\r\n\r\n# A tibble: 55,080 x 3\r\n   topic term     beta\r\n   <int> <chr>   <dbl>\r\n 1     1 a     0.0268 \r\n 2     2 a     0.0200 \r\n 3     3 a     0.0247 \r\n 4     4 a     0.00110\r\n 5     5 a     0.0187 \r\n 6     6 a     0.00411\r\n 7     7 a     0.0196 \r\n 8     8 a     0.0304 \r\n 9     9 a     0.0273 \r\n10    10 a     0.00303\r\n# ... with 55,070 more rows\r\n\r\n\r\n\r\nblogGamma<-tidy(blogLDA, matrix = \"gamma\")%>%group_by(document)\r\nblogGamma %>%\r\n  mutate(title = reorder(document, gamma * topic)) %>%\r\n  ggplot(aes(factor(topic), gamma)) +\r\n  geom_boxplot() +\r\n  ggtitle(\"Blog:Topic Distribution over time\")+\r\n  facet_wrap(~ title)\r\n\r\n\r\n\r\n\r\n‘This graph help us to determine the influence of each event.It is determined significant topic when their gamma score is greater than 0.3. At 5 pm, topic 6,9,19 are the ones. Extract useful words from topic them,we can determine it is pokrally (start of event 5).At 6pm, topic 13 shoots to the top and reaches peak among the 4 hours. The word “abila” raised the attention. They are participants for event 2 (fire).At 6pm, topic 3,8,10 are hot topics. Key words are “pokrally” and “kronosstar”.At 7pm, topic 1 and 13 are trendy topics which has the same content as previous.’\r\n\r\n\r\nTopic <- topicProb %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n =5) %>% \r\n  ungroup() %>%\r\n  arrange(topic, -beta)\r\n\r\n\r\n\r\n\r\n\r\nTopic %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  scale_y_reordered()\r\n\r\n\r\n\r\n  warnings()\r\n\r\n\r\n\r\nRepeat the same step for calls.\r\n\r\n\r\n#create new column: ID and Time, bin time for every hour for call\r\ncall$ID <- seq.int(nrow(call))\r\ncall$time_bin = cut(call$`date(yyyyMMddHHmmss)`, breaks=\"60 mins\")\r\ncall$time_bin<-call$time_bin %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\ncall_topic<-call%>%\r\n  group_by(time_bin) %>% \r\n  unnest_tokens(word, message) %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n\r\nword cloud by hour for call\r\n\r\n\r\nset.seed(1234)\r\ncall_topic %>%\r\n  group_by(time_bin) %>% \r\n  slice_max(order_by = n, n = 20) %>% \r\nggplot(aes(label = word,size = n,col = as.character(n))) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  ggtitle(\"Call:Word Cloud by hour\")+\r\n  facet_wrap(~time_bin)\r\n\r\n\r\n\r\n\r\n\r\n\r\ncallDTM <-call_topic%>%cast_dtm(time_bin,word,n)\r\ncallLDA <-LDA(callDTM, k = 20, control = list(seed = 1234))\r\ntopicProb <- tidy(callLDA, matrix = \"beta\")\r\ntopicProb\r\n\r\n\r\n# A tibble: 2,100 x 3\r\n   topic term     beta\r\n   <int> <chr>   <dbl>\r\n 1     1 stop  0.138  \r\n 2     2 stop  0.0953 \r\n 3     3 stop  0.208  \r\n 4     4 stop  0.0863 \r\n 5     5 stop  0.110  \r\n 6     6 stop  0.00461\r\n 7     7 stop  0.0891 \r\n 8     8 stop  0.0141 \r\n 9     9 stop  0.188  \r\n10    10 stop  0.0210 \r\n# ... with 2,090 more rows\r\n\r\ncallGamma<-tidy(callLDA, matrix = \"gamma\")%>%group_by(document)\r\ncallGamma %>%\r\n  mutate(title = reorder(document, gamma * topic)) %>%\r\n  ggplot(aes(factor(topic), gamma)) +\r\n  geom_boxplot() +\r\n  ggtitle(\"Call:Topic Distribution over time\")+\r\n  facet_wrap(~ title)\r\n\r\n\r\n\r\n\r\nQn2:Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.\r\nZoom in to analyze the changes in the minute unit instead of hour.\r\n\r\n\r\nblog$mins = cut(blog$`date(yyyyMMddHHmmss)`, breaks=\"10 min\")\r\nblog$mins<-blog$mins %>% str_replace_all(\"2014-01-23\",\"\")\r\n\r\n\r\n\r\n\r\n\r\nblog%>%\r\n  group_by(mins) %>%\r\n  summarise(number = n()) %>%\r\n  ggplot(aes(y=number, x=mins)) +\r\n  geom_point(color='red',size=3)+\r\n  ggtitle(\"No of Blog over time by minutes\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nFALSE\r\n\r\n\r\n[1] FALSE\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-14-vast-2021-mc-3/vast-2021-mc-3_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-07-25T22:38:24+08:00",
    "input_file": "vast-2021-mc-3.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to BizMakeover 2",
    "description": {},
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T13:21:06+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-06-20-the-sharpe-ratio/",
    "title": "Biz Makeover2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Li Shuxian",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1.0 Critique of Visualisation\r\n1.1 Clarity\r\n1.2 Aesthetic\r\n\r\n2.0 Alternative Design\r\n2.2 Aesthetic\r\n\r\n3.0 Proposed Visualisation\r\n4.0 Step-by-step Guide\r\n5.0 Derived Insights\r\n\r\n1.0 Critique of Visualisation\r\nThe original visualisation is shown below.\r\n\r\n1.1 Clarity\r\nNo title and legend in the graph Without title of graph, readers cannot extract the main objective of the graph. There is no specific illustration about time frame of the data extracted from the graph.\r\nInconsistent axis header Names of both y-axis and x-axis are inconsistent. For instance, there are two headers in x-axis: “NET EXPORTS” and “Exports”. Reader might be confused which value it is.\r\nUnclear of derived values There is no explanation of calculation of net import and net export.\r\nUnclear source of comment There is short paragraph describe trading in Singapore between China and United States in the bottom of picture. Those information cannot be observed from the graph which lacks evidence.\r\n1.2 Aesthetic\r\nToo many colors in the graph There are ten countries in the graph, each of them has one color but they stand for the same meaning: total number of trading volume in the year. Rainbow colors should be avoided.\r\nRepeating monetary units Every amount in the graph has a monetary unit S$ xx Bil in the graph. It was repeated for ten times.\r\nFull opacity There are areas where countries overlap each other, due to full opacity, it is covered and block reader’s from full picture of the graph.\r\nConfusing position of top net importer and top net exporter sign Sign of top net importer and top net exporter are positioned above countries. The shape of the sign is circle which looks like countries whose sign is also circle. Readers might treat them as countries at a glance.\r\n2.0 Alternative Design\r\nThe proposed design is shown in the following.\r\n ## 2.1 Clarity\r\nInteractive panel allows audience to choose countries select and animations allow readers to see how trading volume changes over time.\r\nBox plot is used to show uncertainty distribution variability over time.\r\nOutliers can show clearly what are months trading falls out of normal range and it can be reflected in responding heat map.\r\n2.2 Aesthetic\r\nAnnotations are used to emphases key observations.\r\nBright and contrastive colors are applied so that readers can focus on their interested category.\r\n3.0 Proposed Visualisation\r\nFor a clearer graph, please click the link on Tableau Public here.\r\n4.0 Step-by-step Guide\r\nOpen raw data using excel.\r\n\r\nRemove first 5 rows in both spreadsheet T1 and T2 to ensure header is in first row.\r\n\r\nRemove row 2 to row 8 in both sheet T1 and T2, since those rows are cumulative values for continents.\r\n\r\nDelete row 115 to 128 in sheet T1 as they are commentary. Similarly, repeat that in T2.\r\n\r\nRemove column B to PE since requested period is between January 2011 to December 2020 in both sheets. Similarly, remove data from January 2021 to April 2021 in both sheets.\r\n\r\nInsert a new column in sheet T1, name it as “Type”, fill the rows with “Import”. Repeat the same procedure in sheet T2 except the name is “Export” instead.\r\n\r\nSet range of table to better insert into tableau.\r\n\r\n\r\nSelect variables and type and unpivot rest of columns.\r\n\r\nRename the 3rd and 4th columns into month and trade value, respectively.\r\n\r\nClose and load the table, a new table 1 will appear, rename the sheet as import.\r\n\r\nMultiply trade value by 1000 to reflect real trading amount.\r\n\r\nRepeat step g to k in export sheet as well. Combine these 2 tables into a new excel workbook. Inner join the table by country name and month of the year.\r\n\r\nRename the variables and change datatype of month from string to date.\r\n\r\ncreate new variables Net Import and Net Export using calculation field.\r\n\r\nCreate upper and lower bond for net export.\r\n\r\nDrag lower and upper bound to measure values.\r\n\r\nPut month in column, sum of net export and measure values in rows. Filter by country and measure names.\r\n\r\nAdd average reference line, change the format and add the customized label.\r\n\r\nCreate new variable Outlier and drag it into color pane.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nChart: Net Export time series with outliers is completed shown as such. Repeat the same step to create Net Import chart.\r\n\r\nCreate a new sheet, drag month of year to column and year in row to create a heatmap.\r\n\r\nDrag net export to color and measure names to detail.\r\n\r\nApply filter by country. A heat map is created completely.\r\n\r\nTo show uncertainties of net import over time, box plot is created. Drag year to columns, net export as rows.\r\n\r\nCreate 2 new variables, positive/negative net export using if condition and drag them into tooltip.\r\n\r\nAdd filter by country and measure names as well. Drag month of year and measure names into tooltip.\r\n\r\nEdit tooltip where net export with negative value is blue color while positive one is red.\r\n\r\nAdjust size of circle and reduce opacity.\r\n\r\nEdit title of graph, insert variable country.\r\n\r\nBox plot is completed as shown.\r\n\r\nCreate ranking variables using rank_dense for net export and also net import.\r\n\r\nCreate a new sheet, drag country to column and net export to rows.\r\n\r\nApply year, net export and country (to select multiple countries) to filter.\r\n\r\nDrag export trade value, import trade value, net import ranking, net export ranking, net export, positive net export, negative net import, year into detail. Choose net export to color pane.\r\n\r\nClick sort to rank them in descending order.\r\n\r\nEdit tooltip as below.\r\n\r\nThe final main panel is shown.\r\n\r\nOpen a dashboard to drag all these graphs inside, select automatic size.\r\n\r\nSwitch on animation for all sheets.\r\n\r\nShow filter of countries, select worksheet it applies.\r\n\r\nShow filter of year and multiple countries, select single value for first country and period.\r\n\r\n\r\nShow highlight of all field.\r\n\r\nHide time series header for net import.\r\n\r\nThe final dashboard is ready. \r\n5.0 Derived Insights\r\nMainland China: The net export results from January 2011 to December 2018 shows an average performance between $1,221,107,952 and -$38,569,636. There was a sharp decreased in net export in January 2019 which rates about -$1,178,839,000. This sharp fall bounced right back to $1,832,598,000 on the following month, February 2019. Outstanding increase of net export value also shown in August 2019 which performed a highest rate among all the years about $2,077,969,000. \r\nUnited States of America:  Back in January 2011 to January 2020, there is an underperformance net exports shown in the graph which range between -$511,229,502 to -$1,996,866,464. In the year of 2020, U.S net export values increased from -$568.861,000 to $2 billion. This outstanding performance started from January 2020 to July 2020. \r\nThailand:  Starting from January 2011, Thailand has an average performance of their net export values till September 2019. The performance increased to $1.4 billion from September 2019 to March 2020. However, the following month in April 2020, there was a noticeable drop to -$5.6 million.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-20T16:55:11+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
